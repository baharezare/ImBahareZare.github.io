<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project: MNIST Digit Classification with SVM</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <h1>Project: MNIST Digit Classification with SVM</h1>
    </header>
    <main>
        <section>
            <h2>Overview</h2>
            <p>This project explores digit classification using the MNIST dataset. The objective is to classify handwritten digits (0â€“9) using Support Vector Machines (SVM) with different kernels and hyperparameter tuning strategies. The results demonstrate the importance of kernel selection and hyperparameter optimization for improving model accuracy.</p>
        </section>

        <section>
            <h2>Tools and Libraries</h2>
            <p>The project utilizes the following tools:</p>
            <ul>
                <li><strong>Scikit-learn</strong>: For building and training SVM models.</li>
                <li><strong>NumPy</strong>: For numerical computation.</li>
                <li><strong>Matplotlib</strong>: For data visualization.</li>
                <li><strong>Python</strong>: As the programming language for implementing the models.</li>
            </ul>
        </section>

        <section>
            <h2>Dataset Description</h2>
            <p>The MNIST dataset contains 70,000 images of handwritten digits, each represented as a 28x28 grayscale image:</p>
            <ul>
                <li><strong>Features:</strong> Flattened 784-pixel intensity values for each image.</li>
                <li><strong>Labels:</strong> Digits from 0 to 9.</li>
            </ul>
            <p>Here is a sample image from the dataset:</p>
            <img src="images/mnist.png" alt="MNIST Sample" style="width:100%; max-width:200px;">
        </section>

        <section>
            <h2>Methodology</h2>
            <p>The project followed these steps:</p>
            <ol>
                <li>Loaded and normalized the MNIST dataset.</li>
                <li>Split the data into training (6000 samples) and testing (1000 samples) subsets.</li>
                <li>Trained SVM models with:
                    <ul>
                        <li>Linear kernel: Achieved an accuracy of <strong>91.8%</strong>.</li>
                        <li>RBF kernel (default parameters): Achieved an accuracy of <strong>94.8%</strong>.</li>
                    </ul>
                </li>
                <li>Optimized hyperparameters using:
                    <ul>
                        <li>GridSearchCV</li>
                        <li>RandomizedSearchCV</li>
                    </ul>
                </li>
                <li>Compared the performance and computational cost of both search methods.</li>
            </ol>
        </section>

        <section>
            <h2>Results</h2>
            <p>The results highlight the impact of kernel choice and hyperparameter tuning:</p>
            <ul>
                <li><strong>Linear Kernel:</strong> Simple but less accurate (91.8%).</li>
                <li><strong>RBF Kernel:</strong> Higher accuracy (94.8%) with default parameters.</li>
                <li><strong>Hyperparameter Tuning:</strong> Further improvements using GridSearchCV and RandomizedSearchCV, but GridSearch required significantly more time.</li>
            </ul>
        </section>

        <section>
            <h2>Visualization</h2>
            <p>The following is an example of a correctly classified digit using the model:</p>
            <img src="images/mnist_visualization.png" alt="Digit Classification Example" style="width:100%; max-width:200px;">
        </section>

        <section>
            <h2>Insights</h2>
            <p>The project provided the following insights:</p>
            <ul>
                <li>RBF kernels significantly outperform linear kernels for this dataset.</li>
                <li>Hyperparameter tuning can improve accuracy, but computational cost must be considered.</li>
                <li>RandomizedSearchCV is faster than GridSearchCV for large parameter grids.</li>
            </ul>
        </section>

        <section>
            <h2>Link to Code</h2>
            <p>Explore the complete code on <a href="https://github.com/baharezare/Hyperparameter-Tuning-for-SVM" target="_blank">GitHub</a>.</p>
        </section>
    </main>
    <footer>
        <a href="index.html">Back to Portfolio</a>
    </footer>
</body>
</html>
